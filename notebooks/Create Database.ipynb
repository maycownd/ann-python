{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = '../data/banana_vs_avocado.hdf5'  # address to where you want to save the hdf5 file\n",
    "cat_dog_train_path = '../data/*/*.jpg'\n",
    "# read addresses and labels from the 'train' folder\n",
    "addrs = glob.glob(cat_dog_train_path, recursive=True)\n",
    "labels = [0 if 'Banana' in addr else 1 for addr in addrs]  # 0 = Cat, 1 = Dog\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to shuffle data\n",
    "if shuffle_data:\n",
    "    c = list(zip(addrs, labels))\n",
    "    shuffle(c)\n",
    "    addrs, labels = zip(*c)\n",
    "    \n",
    "# Divide the hata into 80% train, 20% validation\n",
    "train_addrs = addrs[0:int(0.8*len(addrs))]\n",
    "train_labels = labels[0:int(0.8*len(labels))]\n",
    "val_addrs = addrs[int(0.8*len(addrs)):]\n",
    "val_labels = labels[int(0.8*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "dt = h5py.special_dtype(vlen=str)     # PY3\n",
    "\n",
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "# check the order of data and chose proper data shape to save images\n",
    "if data_order == 'th':\n",
    "    train_shape = (len(train_addrs), 3, 224, 224)\n",
    "    val_shape = (len(val_addrs), 3, 224, 224)\n",
    "elif data_order == 'tf':\n",
    "    train_shape = (len(train_addrs), 224, 224, 3)\n",
    "    val_shape = (len(val_addrs), 224, 224, 3)\n",
    "# open a hdf5 file and create earrays\n",
    "hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "hdf5_file.create_dataset(\"train_set_x\", train_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"list_classes\", (2,), dtype=dt)\n",
    "hdf5_file.create_dataset(\"val_set_x\", val_shape, np.int8)\n",
    "hdf5_file.create_dataset(\"train_mean\", train_shape[1:], np.float32)\n",
    "hdf5_file.create_dataset(\"train_set_y\", (len(train_addrs),), np.int8)\n",
    "hdf5_file[\"train_set_y\"][...] = train_labels\n",
    "hdf5_file.create_dataset(\"val_set_y\", (len(val_addrs),), np.int8)\n",
    "hdf5_file[\"val_set_y\"][...] = val_labels\n",
    "hdf5_file[\"list_classes\"][...] = ['banana', 'bacate bonito']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10/733\n",
      "Train data: 20/733\n",
      "Train data: 30/733\n",
      "Train data: 40/733\n",
      "Train data: 50/733\n",
      "Train data: 60/733\n",
      "Train data: 70/733\n",
      "Train data: 80/733\n",
      "Train data: 90/733\n",
      "Train data: 100/733\n",
      "Train data: 110/733\n",
      "Train data: 120/733\n",
      "Train data: 130/733\n",
      "Train data: 140/733\n",
      "Train data: 150/733\n",
      "Train data: 160/733\n",
      "Train data: 170/733\n",
      "Train data: 180/733\n",
      "Train data: 190/733\n",
      "Train data: 200/733\n",
      "Train data: 210/733\n",
      "Train data: 220/733\n",
      "Train data: 230/733\n",
      "Train data: 240/733\n",
      "Train data: 250/733\n",
      "Train data: 260/733\n",
      "Train data: 270/733\n",
      "Train data: 280/733\n",
      "Train data: 290/733\n",
      "Train data: 300/733\n",
      "Train data: 310/733\n",
      "Train data: 320/733\n",
      "Train data: 330/733\n",
      "Train data: 340/733\n",
      "Train data: 350/733\n",
      "Train data: 360/733\n",
      "Train data: 370/733\n",
      "Train data: 380/733\n",
      "Train data: 390/733\n",
      "Train data: 400/733\n",
      "Train data: 410/733\n",
      "Train data: 420/733\n",
      "Train data: 430/733\n",
      "Train data: 440/733\n",
      "Train data: 450/733\n",
      "Train data: 460/733\n",
      "Train data: 470/733\n",
      "Train data: 480/733\n",
      "Train data: 490/733\n",
      "Train data: 500/733\n",
      "Train data: 510/733\n",
      "Train data: 520/733\n",
      "Train data: 530/733\n",
      "Train data: 540/733\n",
      "Train data: 550/733\n",
      "Train data: 560/733\n",
      "Train data: 570/733\n",
      "Train data: 580/733\n",
      "Train data: 590/733\n",
      "Train data: 600/733\n",
      "Train data: 610/733\n",
      "Train data: 620/733\n",
      "Train data: 630/733\n",
      "Train data: 640/733\n",
      "Train data: 650/733\n",
      "Train data: 660/733\n",
      "Train data: 670/733\n",
      "Train data: 680/733\n",
      "Train data: 690/733\n",
      "Train data: 700/733\n",
      "Train data: 710/733\n",
      "Train data: 720/733\n",
      "Train data: 730/733\n",
      "Validation data: 10/184\n",
      "Validation data: 20/184\n",
      "Validation data: 30/184\n",
      "Validation data: 40/184\n",
      "Validation data: 50/184\n",
      "Validation data: 60/184\n",
      "Validation data: 70/184\n",
      "Validation data: 80/184\n",
      "Validation data: 90/184\n",
      "Validation data: 100/184\n",
      "Validation data: 110/184\n",
      "Validation data: 120/184\n",
      "Validation data: 130/184\n",
      "Validation data: 140/184\n",
      "Validation data: 150/184\n",
      "Validation data: 160/184\n",
      "Validation data: 170/184\n",
      "Validation data: 180/184\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# a numpy array to save the mean of the images\n",
    "mean = np.zeros(train_shape[1:], np.float32)\n",
    "# loop over train addresses\n",
    "for i in range(len(train_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if i % 10 == 0 and i > 1:\n",
    "        print ('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = train_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'th':\n",
    "        img = np.rollaxis(img, 2)\n",
    "    # save the image and calculate the mean so far\n",
    "    hdf5_file[\"train_set_x\"][i, ...] = img[None]\n",
    "    mean += img / float(len(train_labels))\n",
    "# loop over validation addresses\n",
    "for i in range(len(val_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if i % 10 == 0 and i > 1:\n",
    "        print ('Validation data: {}/{}'.format(i, len(val_addrs)))\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = val_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    if data_order == 'th':\n",
    "        img = np.rollaxis(img, 2)\n",
    "    # save the image\n",
    "    hdf5_file[\"val_set_x\"][i, ...] = img[None]\n",
    "\n",
    "# save the mean and close the hdf5 file\n",
    "hdf5_file[\"train_mean\"][...] = mean\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtract_mean = False\n",
    "# open the hdf5 file\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file[\"train_mean\"][0, ...]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "# Total number of samples\n",
    "data_num = hdf5_file[\"train_set_x\"].shape[0]\n",
    "data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 74\n",
      "0 [1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 74\n",
      "1 [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 74\n",
      "0 [1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 74\n",
      "1 [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 74\n",
      "1 [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 74\n",
      "1 [0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "batch_size = 10\n",
    "nb_class=2\n",
    "# create list of batches to shuffle the data\n",
    "batches_list = list(range(int(ceil(float(data_num) / batch_size))))\n",
    "shuffle(batches_list)\n",
    "# loop over batches\n",
    "for n, i in enumerate(batches_list):\n",
    "    i_s = i * batch_size  # index of the first image in this batch\n",
    "    i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "    # read batch images and remove training mean\n",
    "    images = hdf5_file[\"train_set_x\"][i_s:i_e, ...]\n",
    "    \n",
    "    if subtract_mean:\n",
    "        images -= mm\n",
    "    # read labels and convert to one hot encoding\n",
    "    labels = hdf5_file[\"train_set_y\"][i_s:i_e]\n",
    "    labels_one_hot = np.zeros((batch_size, nb_class))\n",
    "    labels_one_hot[np.arange(batch_size), labels] = 1\n",
    "    print (n+1, '/', len(batches_list))\n",
    "    print (labels[0], labels_one_hot[0, :])\n",
    "    plt.imshow(images[0])\n",
    "    plt.show()\n",
    "    if n == 5:  # break after 5 batches\n",
    "        break\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DS Talks ANN)",
   "language": "python",
   "name": "ds-talks-ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
